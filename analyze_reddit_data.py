"""
Reddit ë°ì´í„° ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
"""

import json
import pandas as pd
from collections import Counter
from datetime import datetime
from utils.stock_ticker_extractor import StockTickerExtractor

def load_reddit_data(json_file):
    """JSON íŒŒì¼ì—ì„œ Reddit ë°ì´í„° ë¡œë“œ"""
    with open(json_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    return pd.DataFrame(data)

def analyze_keywords(df):
    """í‚¤ì›Œë“œ íŠ¸ë Œë“œ ë¶„ì„"""
    print("\n" + "="*60)
    print("ğŸ“Š í‚¤ì›Œë“œ íŠ¸ë Œë“œ ë¶„ì„")
    print("="*60)
    
    all_keywords = []
    for keywords in df['tech_keywords'].dropna():
        if isinstance(keywords, list):
            all_keywords.extend(keywords)
    
    keyword_counts = Counter(all_keywords)
    top_keywords = keyword_counts.most_common(20)
    
    print(f"\nì´ í‚¤ì›Œë“œ ìˆ˜: {len(all_keywords)}")
    print(f"ê³ ìœ  í‚¤ì›Œë“œ ìˆ˜: {len(keyword_counts)}")
    print("\nìƒìœ„ 20ê°œ í‚¤ì›Œë“œ:")
    for i, (keyword, count) in enumerate(top_keywords, 1):
        print(f"  {i:2d}. {keyword:30s} : {count:4d}íšŒ")
    
    return dict(top_keywords)

def analyze_sentiment(df):
    """ê°ì„± ë¶„ì„"""
    print("\n" + "="*60)
    print("ğŸ˜Š ê°ì„± ë¶„ì„")
    print("="*60)
    
    sentiments = []
    for score in df['sentiment_score'].dropna():
        if isinstance(score, dict):
            sentiments.append(score.get('overall_sentiment', 'neutral'))
    
    sentiment_counts = Counter(sentiments)
    
    print(f"\nì´ ë¶„ì„ëœ í¬ìŠ¤íŠ¸: {len(sentiments)}")
    for sentiment, count in sentiment_counts.most_common():
        percentage = (count / len(sentiments)) * 100
        print(f"  {sentiment:10s}: {count:4d}ê°œ ({percentage:5.1f}%)")
    
    return dict(sentiment_counts)

def analyze_subreddits(df):
    """ì„œë¸Œë ˆë”§ë³„ í†µê³„"""
    print("\n" + "="*60)
    print("ğŸ“± ì„œë¸Œë ˆë”§ë³„ í†µê³„")
    print("="*60)
    
    subreddit_counts = df['subreddit'].value_counts()
    
    print(f"\nì´ ì„œë¸Œë ˆë”§ ìˆ˜: {len(subreddit_counts)}")
    print("\nìƒìœ„ 15ê°œ ì„œë¸Œë ˆë”§:")
    for i, (subreddit, count) in enumerate(subreddit_counts.head(15).items(), 1):
        print(f"  {i:2d}. r/{subreddit:30s} : {count:4d}ê°œ")
    
    return dict(subreddit_counts.head(15))

def analyze_engagement(df):
    """ì°¸ì—¬ë„ ë¶„ì„"""
    print("\n" + "="*60)
    print("ğŸ”¥ ì°¸ì—¬ë„ ë¶„ì„")
    print("="*60)
    
    avg_score = df['score'].mean()
    avg_comments = df['num_comments'].mean()
    avg_upvote_ratio = df['upvote_ratio'].mean()
    
    print(f"\ní‰ê·  ì ìˆ˜ (Score): {avg_score:.1f}")
    print(f"í‰ê·  ëŒ“ê¸€ ìˆ˜: {avg_comments:.1f}")
    print(f"í‰ê·  ì¶”ì²œ ë¹„ìœ¨: {avg_upvote_ratio:.2%}")
    
    # ì¸ê¸° í¬ìŠ¤íŠ¸ Top 10
    print("\nğŸŒŸ ì¸ê¸° í¬ìŠ¤íŠ¸ Top 10 (ì ìˆ˜ ê¸°ì¤€):")
    top_posts = df.nlargest(10, 'score')[['title', 'score', 'num_comments', 'subreddit']]
    for i, row in enumerate(top_posts.itertuples(), 1):
        print(f"\n  {i}. [{row.subreddit}] {row.title[:70]}...")
        print(f"     â¬†ï¸  Score: {row.score} | ğŸ’¬ Comments: {row.num_comments}")
    
    return {
        'avg_score': float(avg_score),
        'avg_comments': float(avg_comments),
        'avg_upvote_ratio': float(avg_upvote_ratio)
    }

def analyze_stock_tickers(df, mode='aggressive'):
    """
    ì£¼ì‹ í‹°ì»¤ ë¶„ì„
    
    Args:
        df: Reddit ë°ì´í„° DataFrame
        mode: 'aggressive' (ëª¨ë“  í‹°ì»¤ ì¶”ì¶œ) ë˜ëŠ” 'strict' (ì•Œë ¤ì§„ í‹°ì»¤ë§Œ)
    """
    print("\n" + "="*60)
    print("ğŸ“ˆ ì£¼ì‹ í‹°ì»¤ ë¶„ì„")
    print("="*60)
    
    if mode == 'aggressive':
        print("ğŸ”“ AGGRESSIVE ëª¨ë“œ: ëª¨ë“  ì£¼ì‹ í‹°ì»¤ ì¶”ì¶œ (ìŠ¤íƒ€íŠ¸ì—… í¬í•¨)")
    else:
        print("ğŸ”’ STRICT ëª¨ë“œ: ì•Œë ¤ì§„ ì£¼ìš” ì£¼ì‹ë§Œ ì¶”ì¶œ")
    
    # í‹°ì»¤ ì¶”ì¶œê¸° ì´ˆê¸°í™”
    extractor = StockTickerExtractor(mode=mode)
    
    # ëª¨ë“  í‹°ì»¤ ìˆ˜ì§‘
    all_tickers = []
    ticker_posts = []  # í‹°ì»¤ë³„ í¬ìŠ¤íŠ¸ ì •ë³´
    
    for idx, row in df.iterrows():
        # stock_tickers ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ì¶”ì¶œ
        if 'stock_tickers' in row and row['stock_tickers']:
            if isinstance(row['stock_tickers'], list):
                tickers = row['stock_tickers']
            else:
                tickers = []
        else:
            # í…ìŠ¤íŠ¸ì—ì„œ í‹°ì»¤ ì¶”ì¶œ
            text = str(row.get('title', '')) + ' ' + str(row.get('content', ''))
            tickers = extractor.extract_tickers(text)
        
        all_tickers.extend(tickers)
        
        # ê° í‹°ì»¤ì— ëŒ€í•œ í¬ìŠ¤íŠ¸ ì •ë³´ ì €ì¥
        for ticker in tickers:
            ticker_posts.append({
                'ticker': ticker,
                'score': row.get('score', 0),
                'num_comments': row.get('num_comments', 0),
                'subreddit': row.get('subreddit', ''),
                'title': row.get('title', ''),
                'sentiment': row.get('sentiment_score', {}).get('overall_sentiment', 'neutral') if isinstance(row.get('sentiment_score'), dict) else 'neutral'
            })
    
    if not all_tickers:
        print("\nâš ï¸  ì¶”ì¶œëœ ì£¼ì‹ í‹°ì»¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return {}
    
    # í‹°ì»¤ë³„ ì¹´ìš´íŠ¸
    ticker_counts = Counter(all_tickers)
    top_tickers = ticker_counts.most_common(30)
    
    print(f"\nì´ í‹°ì»¤ ë©˜ì…˜ ìˆ˜: {len(all_tickers)}")
    print(f"ê³ ìœ  í‹°ì»¤ ìˆ˜: {len(ticker_counts)}")
    print("\nğŸ” ìƒìœ„ 30ê°œ ì–¸ê¸‰ëœ ì£¼ì‹:")
    
    # í‹°ì»¤ë³„ ìƒì„¸ ì •ë³´ ê³„ì‚°
    ticker_details = {}
    for ticker, count in top_tickers:
        ticker_data = [p for p in ticker_posts if p['ticker'] == ticker]
        
        avg_score = sum(p['score'] for p in ticker_data) / len(ticker_data)
        avg_comments = sum(p['num_comments'] for p in ticker_data) / len(ticker_data)
        
        # ê°ì„± ë¶„ì„
        sentiments = [p['sentiment'] for p in ticker_data]
        sentiment_counts = Counter(sentiments)
        dominant_sentiment = sentiment_counts.most_common(1)[0][0]
        
        # ì¹´í…Œê³ ë¦¬
        category = extractor.get_ticker_category(ticker)
        
        ticker_details[ticker] = {
            'count': count,
            'avg_score': avg_score,
            'avg_comments': avg_comments,
            'sentiment': dominant_sentiment,
            'category': category,
            'subreddits': list(set(p['subreddit'] for p in ticker_data))
        }
    
    # ì¶œë ¥
    for i, (ticker, count) in enumerate(top_tickers, 1):
        details = ticker_details[ticker]
        sentiment_emoji = {'positive': 'ğŸ˜Š', 'neutral': 'ğŸ˜', 'negative': 'ğŸ˜¢'}.get(details['sentiment'], 'ğŸ˜')
        
        print(f"  {i:2d}. ${ticker:6s} : {count:4d}íšŒ | "
              f"í‰ê·  ì ìˆ˜: {details['avg_score']:6.1f} | "
              f"í‰ê·  ëŒ“ê¸€: {details['avg_comments']:5.1f} | "
              f"{sentiment_emoji} {details['sentiment']:8s} | "
              f"{details['category']}")
    
    # ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„
    print("\nğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ ì–¸ê¸‰ ë¹ˆë„:")
    category_counts = Counter()
    for ticker, details in ticker_details.items():
        category_counts[details['category']] += details['count']
    
    for category, count in category_counts.most_common():
        print(f"  {category:20s}: {count:4d}íšŒ")
    
    return dict(ticker_details)

def create_visualizations(keyword_data, sentiment_data, subreddit_data, stock_ticker_data, output_dir='visualizations'):
    """ì‹œê°í™” ìƒì„± (matplotlib í•„ìš”)"""
    try:
        import matplotlib.pyplot as plt
        import os
        os.makedirs(output_dir, exist_ok=True)
        
        # 1. í‚¤ì›Œë“œ ì°¨íŠ¸
        plt.figure(figsize=(14, 8))
        keywords = list(keyword_data.keys())[:15]
        counts = list(keyword_data.values())[:15]
        
        plt.barh(range(len(keywords)), counts, color='skyblue')
        plt.yticks(range(len(keywords)), keywords)
        plt.xlabel('Frequency', fontsize=12)
        plt.title('Top Technology Keywords (Top 15)', fontsize=14, fontweight='bold')
        plt.tight_layout()
        plt.savefig(f'{output_dir}/keywords.png', dpi=300, bbox_inches='tight')
        print(f"\nâœ… í‚¤ì›Œë“œ ì°¨íŠ¸ ì €ì¥: {output_dir}/keywords.png")
        plt.close()
        
        # 2. ê°ì„± ë¶„ì„ ì°¨íŠ¸
        plt.figure(figsize=(10, 8))
        labels = list(sentiment_data.keys())
        sizes = list(sentiment_data.values())
        colors = {'positive': '#66ff99', 'neutral': '#99ccff', 'negative': '#ff9999'}
        chart_colors = [colors.get(label, '#cccccc') for label in labels]
        
        plt.pie(sizes, labels=labels, colors=chart_colors, autopct='%1.1f%%', 
                startangle=90, textprops={'fontsize': 12})
        plt.title('Sentiment Distribution', fontsize=14, fontweight='bold')
        plt.axis('equal')
        plt.tight_layout()
        plt.savefig(f'{output_dir}/sentiment.png', dpi=300, bbox_inches='tight')
        print(f"âœ… ê°ì„± ë¶„ì„ ì°¨íŠ¸ ì €ì¥: {output_dir}/sentiment.png")
        plt.close()
        
        # 3. ì„œë¸Œë ˆë”§ ì°¨íŠ¸
        plt.figure(figsize=(14, 8))
        subreddits = list(subreddit_data.keys())
        counts = list(subreddit_data.values())
        
        plt.barh(range(len(subreddits)), counts, color='coral')
        plt.yticks(range(len(subreddits)), [f'r/{s}' for s in subreddits])
        plt.xlabel('Number of Posts', fontsize=12)
        plt.title('Posts by Subreddit (Top 15)', fontsize=14, fontweight='bold')
        plt.tight_layout()
        plt.savefig(f'{output_dir}/subreddits.png', dpi=300, bbox_inches='tight')
        print(f"âœ… ì„œë¸Œë ˆë”§ ì°¨íŠ¸ ì €ì¥: {output_dir}/subreddits.png")
        plt.close()
        
        # 4. ì£¼ì‹ í‹°ì»¤ ì°¨íŠ¸
        if stock_ticker_data:
            plt.figure(figsize=(16, 10))
            # ìƒìœ„ 20ê°œë§Œ í‘œì‹œ
            sorted_tickers = sorted(stock_ticker_data.items(), 
                                   key=lambda x: x[1]['count'], 
                                   reverse=True)[:20]
            
            tickers = [f"${t[0]}" for t in sorted_tickers]
            counts = [t[1]['count'] for t in sorted_tickers]
            
            # ê°ì„±ì— ë”°ë¥¸ ìƒ‰ìƒ
            colors_list = []
            for ticker, data in sorted_tickers:
                sentiment = data.get('sentiment', 'neutral')
                if sentiment == 'positive':
                    colors_list.append('#66ff99')
                elif sentiment == 'negative':
                    colors_list.append('#ff9999')
                else:
                    colors_list.append('#99ccff')
            
            bars = plt.barh(range(len(tickers)), counts, color=colors_list)
            plt.yticks(range(len(tickers)), tickers)
            plt.xlabel('Mention Count', fontsize=12)
            plt.title('Top 20 Most Mentioned Stock Tickers', fontsize=14, fontweight='bold')
            
            # ë²”ë¡€ ì¶”ê°€
            from matplotlib.patches import Patch
            legend_elements = [
                Patch(facecolor='#66ff99', label='Positive'),
                Patch(facecolor='#99ccff', label='Neutral'),
                Patch(facecolor='#ff9999', label='Negative')
            ]
            plt.legend(handles=legend_elements, loc='lower right')
            
            plt.tight_layout()
            plt.savefig(f'{output_dir}/stock_tickers.png', dpi=300, bbox_inches='tight')
            print(f"âœ… ì£¼ì‹ í‹°ì»¤ ì°¨íŠ¸ ì €ì¥: {output_dir}/stock_tickers.png")
            plt.close()
            
            # 5. ì¹´í…Œê³ ë¦¬ë³„ ì°¨íŠ¸
            category_counts = {}
            for ticker, data in stock_ticker_data.items():
                category = data.get('category', 'Other')
                category_counts[category] = category_counts.get(category, 0) + data['count']
            
            if category_counts:
                plt.figure(figsize=(12, 8))
                categories = list(category_counts.keys())
                counts = list(category_counts.values())
                
                colors_cat = plt.cm.Set3(range(len(categories)))
                plt.pie(counts, labels=categories, colors=colors_cat, autopct='%1.1f%%',
                       startangle=90, textprops={'fontsize': 11})
                plt.title('Stock Mentions by Category', fontsize=14, fontweight='bold')
                plt.axis('equal')
                plt.tight_layout()
                plt.savefig(f'{output_dir}/stock_categories.png', dpi=300, bbox_inches='tight')
                print(f"âœ… ì£¼ì‹ ì¹´í…Œê³ ë¦¬ ì°¨íŠ¸ ì €ì¥: {output_dir}/stock_categories.png")
                plt.close()
                
    except ImportError:
        print("\nâš ï¸  matplotlibê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ì‹œê°í™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        print("   ì‹œê°í™”ë¥¼ ì›í•˜ì‹œë©´: pip install matplotlib")

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    print("\n" + "="*60)
    print("ğŸš€ Reddit ë°ì´í„° íŠ¸ë Œë“œ ë¶„ì„ ì‹œì‘")
    print("="*60)
    
    # ë°ì´í„° ë¡œë“œ
    json_file = 'data/reddit_data.json'
    df = load_reddit_data(json_file)
    
    print(f"\nâœ… ì´ {len(df)}ê°œì˜ Reddit í¬ìŠ¤íŠ¸ ë¡œë“œ ì™„ë£Œ")
    print(f"ğŸ“… ìˆ˜ì§‘ ì‹œê°„: {df['crawled_at'].min()} ~ {df['crawled_at'].max()}")
    
    # ë¶„ì„ ìˆ˜í–‰
    keyword_data = analyze_keywords(df)
    sentiment_data = analyze_sentiment(df)
    subreddit_data = analyze_subreddits(df)
    engagement_data = analyze_engagement(df)
    stock_ticker_data = analyze_stock_tickers(df)
    
    # ì‹œê°í™” ìƒì„± (ì„ íƒì‚¬í•­)
    print("\n" + "="*60)
    print("ğŸ“Š ì‹œê°í™” ìƒì„± ì‹œë„ ì¤‘...")
    print("="*60)
    try:
        create_visualizations(keyword_data, sentiment_data, subreddit_data, stock_ticker_data)
    except Exception as e:
        print(f"âš ï¸  ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")
    
    # ë¦¬í¬íŠ¸ ì €ì¥ (numpy íƒ€ì…ì„ Python íƒ€ì…ìœ¼ë¡œ ë³€í™˜)
    def convert_to_native(obj):
        """numpy/pandas íƒ€ì…ì„ Python native íƒ€ì…ìœ¼ë¡œ ë³€í™˜"""
        import numpy as np
        if isinstance(obj, (np.integer, np.int64)):
            return int(obj)
        elif isinstance(obj, (np.floating, np.float64)):
            return float(obj)
        elif isinstance(obj, dict):
            return {k: convert_to_native(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [convert_to_native(item) for item in obj]
        return obj
    
    report = {
        'analysis_date': datetime.now().isoformat(),
        'total_posts': int(len(df)),
        'keyword_trends': convert_to_native(keyword_data),
        'sentiment_distribution': convert_to_native(sentiment_data),
        'top_subreddits': convert_to_native(subreddit_data),
        'engagement_metrics': convert_to_native(engagement_data),
        'stock_tickers': convert_to_native(stock_ticker_data)
    }
    
    import os
    os.makedirs('reports', exist_ok=True)
    with open('reports/reddit_trend_report.json', 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print("\n" + "="*60)
    print("âœ… ë¶„ì„ ì™„ë£Œ!")
    print("="*60)
    print(f"ğŸ“„ ë¦¬í¬íŠ¸ ì €ì¥: reports/reddit_trend_report.json")
    print(f"ğŸ“Š ì‹œê°í™” ì €ì¥: visualizations/ ë””ë ‰í† ë¦¬")

if __name__ == '__main__':
    main()

