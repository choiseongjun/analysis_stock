"""
Reddit ë°ì´í„°ì—ì„œ ì£¼ì‹ í‹°ì»¤ ì¶”ì¶œ ì „ìš© ìŠ¤í¬ë¦½íŠ¸

ê¸°ì¡´ reddit_data.json íŒŒì¼ì„ ì½ì–´ì„œ ì£¼ì‹ í‹°ì»¤ë¥¼ ì¶”ì¶œí•˜ê³  ë¶„ì„í•©ë‹ˆë‹¤.
"""

import json
import sys
from collections import Counter
from utils.stock_ticker_extractor import StockTickerExtractor

def main(mode='aggressive'):
    print("="*70)
    print("ğŸ’° Reddit ì£¼ì‹ í‹°ì»¤ ì¶”ì¶œ ë° ë¶„ì„")
    print("="*70)
    
    if mode == 'aggressive':
        print("\nğŸ”“ AGGRESSIVE ëª¨ë“œ: ëª¨ë“  ì£¼ì‹ í‹°ì»¤ ì¶”ì¶œ (ì•Œë ¤ì§€ì§€ ì•Šì€ ìŠ¤íƒ€íŠ¸ì—… í¬í•¨)")
        print("   - ì£¼ì‹ ê´€ë ¨ ë¬¸ë§¥ì— ë‚˜íƒ€ë‚˜ëŠ” ëª¨ë“  ëŒ€ë¬¸ì í‹°ì»¤ ì¶”ì¶œ")
        print("   - ìŠ¤íƒ€íŠ¸ì—…, IPO, ì†Œí˜•ì£¼ë„ ê°ì§€")
    else:
        print("\nğŸ”’ STRICT ëª¨ë“œ: ì•Œë ¤ì§„ ì£¼ìš” ì£¼ì‹ë§Œ ì¶”ì¶œ")
        print("   - 150ê°œ ì´ìƒì˜ ì£¼ìš” ê¸°ìˆ  ê¸°ì—…ë§Œ ì¶”ì¶œ")
    
    # ë°ì´í„° ë¡œë“œ
    json_file = 'data/reddit_data.json'
    
    try:
        with open(json_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        print(f"\nâœ… {len(data)}ê°œì˜ Reddit í¬ìŠ¤íŠ¸ ë¡œë“œ ì™„ë£Œ")
    except FileNotFoundError:
        print(f"\nâŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {json_file}")
        print("ë¨¼ì € Reddit í¬ë¡¤ë§ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”: python run_crawler.py")
        return
    except Exception as e:
        print(f"\nâŒ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return
    
    # í‹°ì»¤ ì¶”ì¶œê¸° ì´ˆê¸°í™”
    extractor = StockTickerExtractor(mode=mode)
    
    # í‹°ì»¤ ì¶”ì¶œ
    all_tickers = []
    ticker_contexts = {}  # í‹°ì»¤ë³„ ë©˜ì…˜ ì •ë³´
    
    print("\nğŸ” ì£¼ì‹ í‹°ì»¤ ì¶”ì¶œ ì¤‘...")
    for idx, post in enumerate(data):
        text = post.get('title', '') + ' ' + post.get('content', '')
        tickers = extractor.extract_tickers(text)
        
        for ticker in tickers:
            all_tickers.append(ticker)
            
            if ticker not in ticker_contexts:
                ticker_contexts[ticker] = {
                    'posts': [],
                    'subreddits': set(),
                    'total_score': 0,
                    'total_comments': 0
                }
            
            ticker_contexts[ticker]['posts'].append({
                'title': post.get('title', ''),
                'score': post.get('score', 0),
                'num_comments': post.get('num_comments', 0),
                'subreddit': post.get('subreddit', ''),
                'permalink': post.get('permalink', '')
            })
            ticker_contexts[ticker]['subreddits'].add(post.get('subreddit', ''))
            ticker_contexts[ticker]['total_score'] += post.get('score', 0)
            ticker_contexts[ticker]['total_comments'] += post.get('num_comments', 0)
        
        # ì§„í–‰ ìƒí™© í‘œì‹œ
        if (idx + 1) % 100 == 0:
            print(f"  ì²˜ë¦¬ ì¤‘... {idx + 1}/{len(data)} í¬ìŠ¤íŠ¸")
    
    if not all_tickers:
        print("\nâš ï¸  ì£¼ì‹ í‹°ì»¤ê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return
    
    # í†µê³„ ì¶œë ¥
    ticker_counts = Counter(all_tickers)
    print(f"\nğŸ“Š ë¶„ì„ ê²°ê³¼:")
    print(f"  ì´ í‹°ì»¤ ë©˜ì…˜ ìˆ˜: {len(all_tickers):,}")
    print(f"  ê³ ìœ  í‹°ì»¤ ìˆ˜: {len(ticker_counts)}")
    
    # ìƒìœ„ 30ê°œ í‹°ì»¤
    print("\nğŸ† ê°€ì¥ ë§ì´ ì–¸ê¸‰ëœ ì£¼ì‹ TOP 30:")
    print("="*70)
    
    for rank, (ticker, count) in enumerate(ticker_counts.most_common(30), 1):
        context = ticker_contexts[ticker]
        avg_score = context['total_score'] / len(context['posts'])
        avg_comments = context['total_comments'] / len(context['posts'])
        category = extractor.get_ticker_category(ticker)
        
        print(f"\n{rank:2d}. ${ticker} ({category})")
        print(f"    ğŸ“ˆ ë©˜ì…˜ íšŸìˆ˜: {count}íšŒ")
        print(f"    â­ í‰ê·  ì ìˆ˜: {avg_score:.1f}")
        print(f"    ğŸ’¬ í‰ê·  ëŒ“ê¸€: {avg_comments:.1f}")
        print(f"    ğŸ“ ì„œë¸Œë ˆë”§: {', '.join(list(context['subreddits'])[:5])}")
        
        # ëŒ€í‘œ í¬ìŠ¤íŠ¸ 1ê°œ í‘œì‹œ
        top_post = max(context['posts'], key=lambda x: x['score'])
        print(f"    ğŸ”¥ ì¸ê¸° í¬ìŠ¤íŠ¸: {top_post['title'][:60]}...")
        print(f"       â†ª {top_post['permalink']}")
    
    # ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„
    print("\n\nğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ ì–¸ê¸‰ í˜„í™©:")
    print("="*70)
    
    category_stats = {}
    for ticker, count in ticker_counts.items():
        category = extractor.get_ticker_category(ticker)
        if category not in category_stats:
            category_stats[category] = {
                'count': 0,
                'tickers': []
            }
        category_stats[category]['count'] += count
        category_stats[category]['tickers'].append((ticker, count))
    
    for category in sorted(category_stats.keys(), 
                          key=lambda x: category_stats[x]['count'], 
                          reverse=True):
        stats = category_stats[category]
        print(f"\n{category}:")
        print(f"  ì´ ë©˜ì…˜: {stats['count']}íšŒ")
        print(f"  í‹°ì»¤ ì¢…ë¥˜: {len(stats['tickers'])}ê°œ")
        
        # í•´ë‹¹ ì¹´í…Œê³ ë¦¬ ìƒìœ„ 5ê°œ í‹°ì»¤
        top_in_category = sorted(stats['tickers'], key=lambda x: x[1], reverse=True)[:5]
        print(f"  Top 5: {', '.join([f'${t[0]}({t[1]})' for t in top_in_category])}")
    
    # ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
    output = {
        'total_mentions': len(all_tickers),
        'unique_tickers': len(ticker_counts),
        'top_tickers': [
            {
                'ticker': ticker,
                'count': count,
                'category': extractor.get_ticker_category(ticker),
                'avg_score': ticker_contexts[ticker]['total_score'] / len(ticker_contexts[ticker]['posts']),
                'avg_comments': ticker_contexts[ticker]['total_comments'] / len(ticker_contexts[ticker]['posts']),
                'subreddits': list(ticker_contexts[ticker]['subreddits'])
            }
            for ticker, count in ticker_counts.most_common(50)
        ],
        'category_breakdown': {
            category: {
                'count': stats['count'],
                'tickers': [t[0] for t in sorted(stats['tickers'], key=lambda x: x[1], reverse=True)]
            }
            for category, stats in category_stats.items()
        }
    }
    
    import os
    os.makedirs('reports', exist_ok=True)
    
    with open('reports/stock_tickers_report.json', 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, indent=2)
    
    print("\n\nâœ… ë¶„ì„ ì™„ë£Œ!")
    print("="*70)
    print("ğŸ“„ ìƒì„¸ ë¦¬í¬íŠ¸: reports/stock_tickers_report.json")
    print("ğŸ’¡ ì „ì²´ íŠ¸ë Œë“œ ë¶„ì„ì„ ë³´ë ¤ë©´: python analyze_reddit_data.py")
    print("\nğŸ’¡ TIP: ëª¨ë“œ ë³€ê²½")
    print("   - ëª¨ë“  í‹°ì»¤ (ê¸°ë³¸): python extract_stocks_from_reddit.py")
    print("   - ì£¼ìš” ì£¼ì‹ë§Œ: python extract_stocks_from_reddit.py --strict")
    print()

if __name__ == '__main__':
    import argparse
    parser = argparse.ArgumentParser(description='Redditì—ì„œ ì£¼ì‹ í‹°ì»¤ ì¶”ì¶œ')
    parser.add_argument('--strict', action='store_true', 
                       help='ì•Œë ¤ì§„ ì£¼ìš” ì£¼ì‹ë§Œ ì¶”ì¶œ (ê¸°ë³¸ê°’: ëª¨ë“  í‹°ì»¤ ì¶”ì¶œ)')
    args = parser.parse_args()
    
    mode = 'strict' if args.strict else 'aggressive'
    main(mode=mode)

